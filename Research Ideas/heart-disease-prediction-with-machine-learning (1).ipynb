{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1.0 Predicting heart disease using machine learning\n\nThis notebook looks into using various Python-based machine learning and data science libraries in an attempt to build a machine learning model capable of predicting whether or not someone has heart disease based on their medical attributes.\n\nI am going to take the following approach:\n1. Problem definition\n2. Data\n3. Evaluation\n4. Features\n5. Modelling\n6. Experimentation\n\n## 1.1 Problem Definition\n\nIn a statement,\n> Given clinical parameters about a patient, can we predict whether or not they have heart disease?\n\n## 1.2 Data\n\nThe original data came from the Cleavland data from the UCI Machine Learning Repository. https://archive.ics.uci.edu/ml/datasets/heart+Disease\n\nThere is also a version of it available on Kaggle. https://www.kaggle.com/ronitf/heart-disease-uci\n\n## 1.3 Evaluation\n\n> If the outcome can reach 95% accuracy at predicting whether or not a patient has heart disease during the proof of concept, we'll pursue the project.\n\n## 1.4 Features\n\nBelow are the details and descriptions of the data features.\n\n**Create data dictionary**\n\n1. age - age in years\n2. sex - (1 = male; 0 = female)\n3. cp - chest pain type\n    * 0: Typical angina: chest pain related decrease blood supply to the heart\n    * 1: Atypical angina: chest pain not related to heart\n    * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n    * 3: Asymptomatic: chest pain not showing signs of disease\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital) anything above 130-140 is typically cause for concern\n5. chol - serum cholestoral in mg/dl\n    * serum = LDL + HDL + .2 * triglycerides\n    * above 200 is cause for concern\n6. fbs - (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n    * '>126' mg/dL signals diabetes\n7. restecg - resting electrocardiographic results\n    * 0: Nothing to note\n    * 1: ST-T Wave abnormality\n        * can range from mild symptoms to severe problems\n        * signals non-normal heart beat\n    * 2: Possible or definite left ventricular hypertrophy\n        * Enlarged heart's main pumping chamber\n8. thalach - maximum heart rate achieved\n9. exang - exercise induced angina (1 = yes; 0 = no)\n10. oldpeak - ST depression induced by exercise relative to rest looks at stress of heart during excercise unhealthy heart will stress more\n11. slope - the slope of the peak exercise ST segment\n    * 0: Upsloping: better heart rate with excercise (uncommon)\n    * 1: Flatsloping: minimal change (typical healthy heart)\n    * 2: Downslopins: signs of unhealthy heart\n12. ca - number of major vessels (0-3) colored by flourosopy\n    * colored vessel means the doctor can see the blood passing through\n    * the more blood movement the better (no clots)\n13. thal - thalium stress result\n    * 1,3: normal\n    * 6: fixed defect: used to be defect but ok now\n    * 7: reversable defect: no proper blood movement when excercising\n14. target - have disease or not (1=yes, 0=no) (= the predicted attribute)","metadata":{}},{"cell_type":"markdown","source":"# 2.0 Preparing the Tools","metadata":{}},{"cell_type":"code","source":"#!pip install sklearn\n#Import all libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Inline matplotlib to view inside this notebook directly\n%matplotlib inline\n\n#Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Evaluation\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:20.920701Z","iopub.execute_input":"2022-03-17T05:17:20.92115Z","iopub.status.idle":"2022-03-17T05:17:21.5487Z","shell.execute_reply.started":"2022-03-17T05:17:20.921072Z","shell.execute_reply":"2022-03-17T05:17:21.54783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 Load Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/hear2csv/heart (2).csv\")\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:21.549918Z","iopub.execute_input":"2022-03-17T05:17:21.550164Z","iopub.status.idle":"2022-03-17T05:17:21.570192Z","shell.execute_reply.started":"2022-03-17T05:17:21.550133Z","shell.execute_reply":"2022-03-17T05:17:21.56891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.0 Data Exploration (exploratory data analysis or EDA)\n\nThe goal here is to find out more about the data and become a subject matter export on the dataset you're working with. \n\n1. What question(s) are you trying to solve?\n2. What kind of data do we have and how do we treat different types?\n3. What's missing from the data and how do you deal with it?\n4. Where are the outliers and why should you care about them?\n5. How can you add, change or remove features to get more out of your data?","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-17T05:17:21.571515Z","iopub.execute_input":"2022-03-17T05:17:21.572271Z","iopub.status.idle":"2022-03-17T05:17:21.591551Z","shell.execute_reply.started":"2022-03-17T05:17:21.572234Z","shell.execute_reply":"2022-03-17T05:17:21.590604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:21.594292Z","iopub.execute_input":"2022-03-17T05:17:21.594624Z","iopub.status.idle":"2022-03-17T05:17:21.611953Z","shell.execute_reply.started":"2022-03-17T05:17:21.594586Z","shell.execute_reply":"2022-03-17T05:17:21.611193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#how many class of one feature or target\ndf[\"target\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:21.613131Z","iopub.execute_input":"2022-03-17T05:17:21.613491Z","iopub.status.idle":"2022-03-17T05:17:21.621119Z","shell.execute_reply.started":"2022-03-17T05:17:21.61346Z","shell.execute_reply":"2022-03-17T05:17:21.620153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#bar chart\ndf[\"target\"].value_counts().plot(kind='bar', color=[\"salmon\",\"lightblue\"])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-17T05:17:21.622411Z","iopub.execute_input":"2022-03-17T05:17:21.62268Z","iopub.status.idle":"2022-03-17T05:17:21.835617Z","shell.execute_reply.started":"2022-03-17T05:17:21.62265Z","shell.execute_reply":"2022-03-17T05:17:21.834642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this barchart, we can see that more data samples has heart disease.","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:21.837023Z","iopub.execute_input":"2022-03-17T05:17:21.837231Z","iopub.status.idle":"2022-03-17T05:17:21.851391Z","shell.execute_reply.started":"2022-03-17T05:17:21.837205Z","shell.execute_reply":"2022-03-17T05:17:21.850411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check missing values of all features\ndf.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:21.853222Z","iopub.execute_input":"2022-03-17T05:17:21.853811Z","iopub.status.idle":"2022-03-17T05:17:21.86318Z","shell.execute_reply.started":"2022-03-17T05:17:21.853761Z","shell.execute_reply":"2022-03-17T05:17:21.862288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:21.864621Z","iopub.execute_input":"2022-03-17T05:17:21.864976Z","iopub.status.idle":"2022-03-17T05:17:21.918324Z","shell.execute_reply.started":"2022-03-17T05:17:21.864931Z","shell.execute_reply":"2022-03-17T05:17:21.917262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.1 Heart Disease Frequency according to Sex","metadata":{}},{"cell_type":"code","source":"df.sex.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:21.919721Z","iopub.execute_input":"2022-03-17T05:17:21.919985Z","iopub.status.idle":"2022-03-17T05:17:21.929357Z","shell.execute_reply.started":"2022-03-17T05:17:21.919954Z","shell.execute_reply":"2022-03-17T05:17:21.928363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Compare target and sex column\npd.crosstab(df.target, df.sex)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:21.930639Z","iopub.execute_input":"2022-03-17T05:17:21.931542Z","iopub.status.idle":"2022-03-17T05:17:21.953241Z","shell.execute_reply.started":"2022-03-17T05:17:21.931476Z","shell.execute_reply":"2022-03-17T05:17:21.952321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create plot of crosstab\npd.crosstab(df.target, df.sex).plot(kind=\"bar\",figsize=(10,6),color=[\"salmon\",\"lightblue\"])\nplt.title(\"Heart Disease Frerquency for Sex\")\nplt.xlabel(\"0 = No Disease, 1=Disease\")\nplt.ylabel(\"Amount\")\nplt.legend([\"Female\",\"Male\"]);\nplt.xticks(rotation=0);","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-17T05:17:21.954661Z","iopub.execute_input":"2022-03-17T05:17:21.95492Z","iopub.status.idle":"2022-03-17T05:17:22.19322Z","shell.execute_reply.started":"2022-03-17T05:17:21.954888Z","shell.execute_reply":"2022-03-17T05:17:22.192217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the barchart above, the frequency of female getting heart disease is higher in this dataset compared to male. ","metadata":{}},{"cell_type":"markdown","source":"## 3.2 Age vs. Max Heart Rate for Heart Disease","metadata":{}},{"cell_type":"code","source":"#Create new figure\nplt.figure(figsize=(10,6))\n\n#Scatter with positive examples\nplt.scatter(df.age[df.target==1],\n           df.thalach[df.target==1],\n           c=\"salmon\")\n\n#Scatter with negative examples\nplt.scatter(df.age[df.target==0],\n           df.thalach[df.target==0],\n           c=\"lightblue\");\n\n#Add some helpful info\nplt.title(\"Heart Disease in function of Age and Max Heart Rate\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max Heart Rate\")\nplt.legend([\"Disease\",\"No Disease\"])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-17T05:17:22.197083Z","iopub.execute_input":"2022-03-17T05:17:22.197334Z","iopub.status.idle":"2022-03-17T05:17:22.50754Z","shell.execute_reply.started":"2022-03-17T05:17:22.197304Z","shell.execute_reply":"2022-03-17T05:17:22.506681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The chances of getting maximum heart rate is higher for heart disease patients. ","metadata":{}},{"cell_type":"code","source":"#Check the distribution of the age column with a histogram\n#May check for outliers of the data\ndf.age.plot.hist()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-17T05:17:22.508888Z","iopub.execute_input":"2022-03-17T05:17:22.509146Z","iopub.status.idle":"2022-03-17T05:17:22.734949Z","shell.execute_reply.started":"2022-03-17T05:17:22.509113Z","shell.execute_reply":"2022-03-17T05:17:22.734164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this histogram, we can see that approximately half of the samples age is between 55 to 65 years old. The rest are from 40s to 70s. There are also a few samples for 30-40 and 70 above.","metadata":{}},{"cell_type":"markdown","source":"## 3.3 Heart Disease Frequency per Chest Pain Type\n3. cp - chest pain type\n    * 0: Typical angina: chest pain related decrease blood supply to the heart\n    * 1: Atypical angina: chest pain not related to heart\n    * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n    * 3: Asymptomatic: chest pain not showing signs of disease","metadata":{}},{"cell_type":"code","source":"pd.crosstab(df.cp,df.target)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:22.73626Z","iopub.execute_input":"2022-03-17T05:17:22.737208Z","iopub.status.idle":"2022-03-17T05:17:22.759074Z","shell.execute_reply.started":"2022-03-17T05:17:22.737152Z","shell.execute_reply":"2022-03-17T05:17:22.757828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make corsstab visualise\npd.crosstab(df.cp,df.target).plot(kind=\"bar\",figsize=(10,6),color=[\"salmon\",\"lightblue\"])\n\nplt.title(\"Heart Disease Frequency Per Chest Pain Type\")\nplt.xlabel(\"CHest Pain Type\")\nplt.ylabel(\"Amount\")\nplt.legend([\"No Disease\",\"Disease\"])\nplt.xticks(rotation=0)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-17T05:17:22.76056Z","iopub.execute_input":"2022-03-17T05:17:22.760833Z","iopub.status.idle":"2022-03-17T05:17:23.02396Z","shell.execute_reply.started":"2022-03-17T05:17:22.760794Z","shell.execute_reply":"2022-03-17T05:17:23.022808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most of the heart disease patients are suffering on the third chest pain type which is non-aginal pain and some of suffering on the first the chest pain type, typical angina and atypical angina.\nAlthough the second and third chest pain type is non related to heart, the data shows patients will suffer on those chest pain types. To make a conclusion, we might need to approach some healthcare professions to ask for their opinions.","metadata":{}},{"cell_type":"markdown","source":"## 3.4 Correlation\nTo see the relationship among the features.","metadata":{}},{"cell_type":"code","source":"#Make a correlation matrix\ndf.corr()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:23.025458Z","iopub.execute_input":"2022-03-17T05:17:23.025721Z","iopub.status.idle":"2022-03-17T05:17:23.054502Z","shell.execute_reply.started":"2022-03-17T05:17:23.025689Z","shell.execute_reply":"2022-03-17T05:17:23.053626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualise correlation\ncorr_matrix = df.corr()\nfig,ax = plt.subplots(figsize=(15,10))\nax=sns.heatmap(corr_matrix,annot=True,linewidths=0.5,fmt=\".2f\",cmap=\"YlGnBu\");\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top-0.5)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-17T05:17:23.056089Z","iopub.execute_input":"2022-03-17T05:17:23.05634Z","iopub.status.idle":"2022-03-17T05:17:24.490953Z","shell.execute_reply.started":"2022-03-17T05:17:23.056307Z","shell.execute_reply":"2022-03-17T05:17:24.490043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1. Positive correlation, both variables increase or decrease in same direction\n2. Negative correlation, one variable increase and one variable decrease vice versa\n\nResult1: Chest pain and target have positive correlation\n-> Higher chest pain level, more targets may get heart disease\n","metadata":{}},{"cell_type":"markdown","source":"# 4.0 Modelling\n\n## Split data to training and test sets","metadata":{}},{"cell_type":"code","source":"#Split data into X and y for training features and target variable\nX=df.drop(\"target\",axis=1)\ny=df[\"target\"]","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:24.49284Z","iopub.execute_input":"2022-03-17T05:17:24.493411Z","iopub.status.idle":"2022-03-17T05:17:24.500153Z","shell.execute_reply.started":"2022-03-17T05:17:24.493362Z","shell.execute_reply":"2022-03-17T05:17:24.498941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Split data into train and test sets\nnp.random.seed(42)\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:24.502075Z","iopub.execute_input":"2022-03-17T05:17:24.502356Z","iopub.status.idle":"2022-03-17T05:17:24.515447Z","shell.execute_reply.started":"2022-03-17T05:17:24.502326Z","shell.execute_reply":"2022-03-17T05:17:24.514434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.1 Train base models\n### 3 different Machine Learning models:\n1. Logistic Regression\n2. K-Nearest Neighbors Classifiers\n3. Random Forest Classifiers\n","metadata":{}},{"cell_type":"code","source":"#Put models in a dictionary\n\nmodels = {\"Logistic Regression\": LogisticRegression(),\n         \"KNN\": KNeighborsClassifier(),\n         \"Random Forest\": RandomForestClassifier()}\n\n#Create function to fit and score models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of differetn Scikit-Learn machine learning models\n    X_train : training data (no labels)\n    X_test : testing data (no labels)\n    y_train : training labels\n    y_test : test labels\n    \"\"\"\n    #set random seed\n    np.random.seed(42)\n    #dictionary to keep model scores\n    model_scores = {}\n    #loop thru models\n    for name, model in models.items():\n        #fit model\n        model.fit(X_train, y_train)\n        #evaluate model and append score\n        model_scores[name]=model.score(X_test, y_test)\n    return model_scores","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:24.517148Z","iopub.execute_input":"2022-03-17T05:17:24.517788Z","iopub.status.idle":"2022-03-17T05:17:24.529292Z","shell.execute_reply.started":"2022-03-17T05:17:24.517724Z","shell.execute_reply":"2022-03-17T05:17:24.528559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_scores = fit_and_score(models=models, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\nmodel_scores","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-17T05:17:24.530789Z","iopub.execute_input":"2022-03-17T05:17:24.531829Z","iopub.status.idle":"2022-03-17T05:17:24.778721Z","shell.execute_reply.started":"2022-03-17T05:17:24.531772Z","shell.execute_reply":"2022-03-17T05:17:24.777804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.1.1 Base Model Comparison","metadata":{}},{"cell_type":"code","source":"model_compare = pd.DataFrame(model_scores, index=[\"accuracy\"])\nmodel_compare.T.plot.bar()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:24.779934Z","iopub.execute_input":"2022-03-17T05:17:24.780159Z","iopub.status.idle":"2022-03-17T05:17:24.986474Z","shell.execute_reply.started":"2022-03-17T05:17:24.780132Z","shell.execute_reply":"2022-03-17T05:17:24.985033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For base model, Logistic Regression and Random Forest works way more better than KNN.","metadata":{}},{"cell_type":"markdown","source":"Let's look at the following:\n* Hypyterparameter tuning\n* Feature importance\n* Confusion matrix\n* Cross-validation\n* Precision\n* Recall\n* F1 score\n* Classification report\n* ROC curve\n* Area under the curve(AUC)\n\n## 4.2 Hyperparameter tuning\n\n### Ways to tune hyperparameters\n1. by hand\n2. RandomizedSearchCV\n3. GridSearchCV\n\n### 4.2.1 Tune by hand","metadata":{}},{"cell_type":"code","source":"#Tune knn\ntrain_scores = []\ntest_scores = []\n\n#list for different values of n-neighbors\nneighbors = range(1,21)\n\n#set up knn instance\nknn = KNeighborsClassifier()\n\n#loop thru list\nfor i in neighbors:\n    knn.set_params(n_neighbors=i)\n    \n    #fit the model\n    knn.fit(X_train, y_train)\n    \n    # Update the training scores list\n    train_scores.append(knn.score(X_train, y_train))\n    \n    # Update the test scores list\n    test_scores.append(knn.score(X_test, y_test))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:24.988029Z","iopub.execute_input":"2022-03-17T05:17:24.988265Z","iopub.status.idle":"2022-03-17T05:17:25.37984Z","shell.execute_reply.started":"2022-03-17T05:17:24.988237Z","shell.execute_reply":"2022-03-17T05:17:25.378922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_scores","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:25.381059Z","iopub.execute_input":"2022-03-17T05:17:25.38131Z","iopub.status.idle":"2022-03-17T05:17:25.389623Z","shell.execute_reply.started":"2022-03-17T05:17:25.381278Z","shell.execute_reply":"2022-03-17T05:17:25.388717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(neighbors, train_scores, label=\"Train score\")\nplt.plot(neighbors, test_scores, label=\"Test score\")\nplt.xticks(np.arange(1, 21, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:25.392389Z","iopub.execute_input":"2022-03-17T05:17:25.392999Z","iopub.status.idle":"2022-03-17T05:17:25.696546Z","shell.execute_reply.started":"2022-03-17T05:17:25.392947Z","shell.execute_reply":"2022-03-17T05:17:25.695523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After tuning the parameter for k value, KNN classifier has imporoved but the performance is still lower than Logistic Regression and Random Forest.","metadata":{"execution":{"iopub.status.busy":"2022-03-17T04:07:58.776192Z","iopub.execute_input":"2022-03-17T04:07:58.776924Z","iopub.status.idle":"2022-03-17T04:07:58.783599Z","shell.execute_reply.started":"2022-03-17T04:07:58.776876Z","shell.execute_reply":"2022-03-17T04:07:58.782305Z"}}},{"cell_type":"markdown","source":"### 4.2.2 Hyperparameter tuning with RandomizedSearchCV\n\nTune: \n* LogisticRegression()\n* RandomForestClassifier()","metadata":{}},{"cell_type":"code","source":"#Create a hyperparameter grid for LR\nlog_reg_grid = {\"C\": np.logspace(-4,4,20),\n                \"solver\": [\"liblinear\"]}\n\n#Create a hyperparameter grid for RF\nrf_grid = {\"n_estimators\": np.arange(10,1000,50),\n          \"max_depth\": [None,3,5,10],\n          \"min_samples_split\": np.arange(2,20,2),\n          \"min_samples_leaf\": np.arange(1,20,2)}","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:25.697988Z","iopub.execute_input":"2022-03-17T05:17:25.698804Z","iopub.status.idle":"2022-03-17T05:17:25.705191Z","shell.execute_reply.started":"2022-03-17T05:17:25.698736Z","shell.execute_reply":"2022-03-17T05:17:25.704207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tune LogisticRegression\n\nnp.random.seed(42)\n\n# Setup random hyperparameter search for LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True)\n\n# Fit random hyperparameter search model for LogisticRegression\nrs_log_reg.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:25.706536Z","iopub.execute_input":"2022-03-17T05:17:25.706896Z","iopub.status.idle":"2022-03-17T05:17:26.292602Z","shell.execute_reply.started":"2022-03-17T05:17:25.70685Z","shell.execute_reply":"2022-03-17T05:17:26.291832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rs_log_reg.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:26.293827Z","iopub.execute_input":"2022-03-17T05:17:26.294041Z","iopub.status.idle":"2022-03-17T05:17:26.299907Z","shell.execute_reply.started":"2022-03-17T05:17:26.294015Z","shell.execute_reply":"2022-03-17T05:17:26.299089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rs_lr_score = rs_log_reg.score(X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:23:07.436177Z","iopub.execute_input":"2022-03-17T05:23:07.437084Z","iopub.status.idle":"2022-03-17T05:23:07.444279Z","shell.execute_reply.started":"2022-03-17T05:23:07.437036Z","shell.execute_reply":"2022-03-17T05:23:07.443165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tune RF\n\n#Set random parameter search for RF\nnp.random.seed(42)\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                            param_distributions=rf_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True)\n#Fit random hyperparameter search model for RF\nrs_rf.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:17:26.316031Z","iopub.execute_input":"2022-03-17T05:17:26.316644Z","iopub.status.idle":"2022-03-17T05:18:53.867655Z","shell.execute_reply.started":"2022-03-17T05:17:26.316588Z","shell.execute_reply":"2022-03-17T05:18:53.866792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Find best hyperparameter\nrs_rf.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:18:53.868915Z","iopub.execute_input":"2022-03-17T05:18:53.869163Z","iopub.status.idle":"2022-03-17T05:18:53.875411Z","shell.execute_reply.started":"2022-03-17T05:18:53.869134Z","shell.execute_reply":"2022-03-17T05:18:53.874536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate the randomised search RF model\nrs_rf_score =rs_rf.score(X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:18:53.876806Z","iopub.execute_input":"2022-03-17T05:18:53.877033Z","iopub.status.idle":"2022-03-17T05:18:53.915817Z","shell.execute_reply.started":"2022-03-17T05:18:53.877004Z","shell.execute_reply":"2022-03-17T05:18:53.914808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#default model scores\nmodel_scores","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:18:53.91748Z","iopub.execute_input":"2022-03-17T05:18:53.917951Z","iopub.status.idle":"2022-03-17T05:18:53.924558Z","shell.execute_reply.started":"2022-03-17T05:18:53.917904Z","shell.execute_reply":"2022-03-17T05:18:53.923723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By using RandomizedSearchCV, the performance of random forest model has improved. But logistic regression model's performance is still higher.","metadata":{}},{"cell_type":"markdown","source":"### 4.2.3 Hyperparameter Tuning with GridSearchCV\nTune: \n* LogisticRegression()","metadata":{}},{"cell_type":"code","source":"#Different hyperparameters for LR model\nlog_reg_grid = {\"C\": np.logspace(-4,4,30),\n               \"solver\": [\"liblinear\"]}\n\n#Setup grid hyperparameter for LR\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                         param_grid=log_reg_grid,\n                         cv=5,\n                         verbose=True)\n\n#Fit into model\ngs_log_reg.fit(X_train,y_train)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:18:53.925959Z","iopub.execute_input":"2022-03-17T05:18:53.926563Z","iopub.status.idle":"2022-03-17T05:18:54.75713Z","shell.execute_reply.started":"2022-03-17T05:18:53.926515Z","shell.execute_reply":"2022-03-17T05:18:54.756173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check best hyperparameters\ngs_log_reg.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:18:54.760497Z","iopub.execute_input":"2022-03-17T05:18:54.76093Z","iopub.status.idle":"2022-03-17T05:18:54.766609Z","shell.execute_reply.started":"2022-03-17T05:18:54.76089Z","shell.execute_reply":"2022-03-17T05:18:54.765644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Evaluate grid search LR model\ngs_lr_score = gs_log_reg.score(X_test,y_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:22:33.991983Z","iopub.execute_input":"2022-03-17T05:22:33.992361Z","iopub.status.idle":"2022-03-17T05:22:34.000357Z","shell.execute_reply.started":"2022-03-17T05:22:33.992321Z","shell.execute_reply":"2022-03-17T05:22:33.999712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_scores.update( [('RandomizedS LR',rs_lr_score),('RandomizedS RF',rs_rf_score),('GridS LR',gs_lr_score)] )\nmodel_compare2 = pd.DataFrame(model_scores, index=[\"accuracy\"])\nmodel_compare2.T.plot.bar(legend=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:34:11.915221Z","iopub.execute_input":"2022-03-17T05:34:11.915551Z","iopub.status.idle":"2022-03-17T05:34:12.156953Z","shell.execute_reply.started":"2022-03-17T05:34:11.915497Z","shell.execute_reply":"2022-03-17T05:34:12.156069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Logistic Regression has the same scores for all of the different hyperparameter tuning own models. \n\nOut of three different classifiers, Logistic Regression has the best performance score during training stage.","metadata":{}},{"cell_type":"markdown","source":"## 4.3 Evaluationg our tuned machine learning classifier, beyond accuracy\n\n* ROC curve and AUC score\n* Confusion matrix\n* Classification report\n* Precision\n* Recall\n* F1-score\n\n... would be great if corss-validation was used where possible\n\nTo make comparisons and evaluate our trained model, first we need to make predictions.","metadata":{}},{"cell_type":"code","source":"#Make predictions with tuned model\ny_preds = gs_log_reg.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:32:27.966786Z","iopub.execute_input":"2022-03-17T05:32:27.967934Z","iopub.status.idle":"2022-03-17T05:32:27.975364Z","shell.execute_reply.started":"2022-03-17T05:32:27.967881Z","shell.execute_reply":"2022-03-17T05:32:27.974227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:32:29.543087Z","iopub.execute_input":"2022-03-17T05:32:29.54338Z","iopub.status.idle":"2022-03-17T05:32:29.548955Z","shell.execute_reply.started":"2022-03-17T05:32:29.543346Z","shell.execute_reply":"2022-03-17T05:32:29.548108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Plot ROC curve and calculate AUC metric\nplot_roc_curve(gs_log_reg, X_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:32:30.574158Z","iopub.execute_input":"2022-03-17T05:32:30.574771Z","iopub.status.idle":"2022-03-17T05:32:30.75019Z","shell.execute_reply.started":"2022-03-17T05:32:30.574705Z","shell.execute_reply":"2022-03-17T05:32:30.749083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Confusion matrix\nprint(confusion_matrix(y_test, y_preds))","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:32:37.45629Z","iopub.execute_input":"2022-03-17T05:32:37.456594Z","iopub.status.idle":"2022-03-17T05:32:37.463391Z","shell.execute_reply.started":"2022-03-17T05:32:37.456559Z","shell.execute_reply":"2022-03-17T05:32:37.462798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set(font_scale=1.5)\n\ndef plot_conf_mat(y_test,y_preds):\n    \"\"\"\n    Plot a nice looking confusion matrix using Seaborn's heatmap\n    \"\"\"\n    fig,ax = plt.subplots(figsize=(3,3))\n    ax=sns.heatmap(confusion_matrix(y_test,y_preds),\n                  annot=True,\n                  cbar=False)\n    plt.xlabel(\"Predicted label\")\n    plt.ylabel(\"True label\")\n    \n    bottom, top = ax.get_ylim()\n    ax.set_ylim(bottom + 0.5, top - 0.5)\n    \nplot_conf_mat(y_test, y_preds)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:32:38.319064Z","iopub.execute_input":"2022-03-17T05:32:38.319493Z","iopub.status.idle":"2022-03-17T05:32:38.441137Z","shell.execute_reply.started":"2022-03-17T05:32:38.31946Z","shell.execute_reply":"2022-03-17T05:32:38.440291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3.1 Classification report\nCross-validated precision, recall and f1-score.","metadata":{}},{"cell_type":"code","source":"#Using only one split\nprint(classification_report(y_test,y_preds))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-17T05:32:40.552735Z","iopub.execute_input":"2022-03-17T05:32:40.553051Z","iopub.status.idle":"2022-03-17T05:32:40.564527Z","shell.execute_reply.started":"2022-03-17T05:32:40.553018Z","shell.execute_reply":"2022-03-17T05:32:40.563832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Calculate evaluation metrics using cross-validation\nusing cross_val_score()","metadata":{}},{"cell_type":"code","source":"#Check best hyperparameters\ngs_log_reg.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:32:42.358146Z","iopub.execute_input":"2022-03-17T05:32:42.358826Z","iopub.status.idle":"2022-03-17T05:32:42.363297Z","shell.execute_reply.started":"2022-03-17T05:32:42.35879Z","shell.execute_reply":"2022-03-17T05:32:42.362762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create a new classifier with best parameters\nclf = LogisticRegression(C=0.20433597178569418,\n                        solver=\"liblinear\")","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:32:48.159152Z","iopub.execute_input":"2022-03-17T05:32:48.159733Z","iopub.status.idle":"2022-03-17T05:32:48.166339Z","shell.execute_reply.started":"2022-03-17T05:32:48.159694Z","shell.execute_reply":"2022-03-17T05:32:48.165308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Cross validated accuracy\ncv_acc= cross_val_score(clf,\n                       X,\n                       y,\n                       cv=5,\n                       scoring=\"accuracy\")\ncv_acc","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:32:53.775122Z","iopub.execute_input":"2022-03-17T05:32:53.775665Z","iopub.status.idle":"2022-03-17T05:32:53.813412Z","shell.execute_reply.started":"2022-03-17T05:32:53.775627Z","shell.execute_reply":"2022-03-17T05:32:53.812801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_acc = np.mean(cv_acc)\ncv_acc","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:32:55.247442Z","iopub.execute_input":"2022-03-17T05:32:55.248337Z","iopub.status.idle":"2022-03-17T05:32:55.253537Z","shell.execute_reply.started":"2022-03-17T05:32:55.248292Z","shell.execute_reply":"2022-03-17T05:32:55.252947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Cross-validated precision\ncv_precision= cross_val_score(clf,\n                       X,\n                       y,\n                       cv=5,\n                       scoring=\"precision\")\ncv_precision = np.mean(cv_precision)\ncv_precision","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:32:58.383178Z","iopub.execute_input":"2022-03-17T05:32:58.383524Z","iopub.status.idle":"2022-03-17T05:32:58.428281Z","shell.execute_reply.started":"2022-03-17T05:32:58.383464Z","shell.execute_reply":"2022-03-17T05:32:58.427648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Cross-validated recall\ncv_recall= cross_val_score(clf,\n                       X,\n                       y,\n                       cv=5,\n                       scoring=\"recall\")\ncv_recall = np.mean(cv_recall)\ncv_recall","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:33:02.375894Z","iopub.execute_input":"2022-03-17T05:33:02.376188Z","iopub.status.idle":"2022-03-17T05:33:02.422959Z","shell.execute_reply.started":"2022-03-17T05:33:02.376157Z","shell.execute_reply":"2022-03-17T05:33:02.421904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Cross-validated f1\ncv_f1= cross_val_score(clf,\n                       X,\n                       y,\n                       cv=5,\n                       scoring=\"f1\")\ncv_f1 = np.mean(cv_f1)\ncv_f1","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:33:03.28738Z","iopub.execute_input":"2022-03-17T05:33:03.287724Z","iopub.status.idle":"2022-03-17T05:33:03.335171Z","shell.execute_reply.started":"2022-03-17T05:33:03.287693Z","shell.execute_reply":"2022-03-17T05:33:03.334263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualise cross-validated metrics\ncv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n                          \"Precision\": cv_precision,\n                          \"Recall\": cv_recall,\n                          \"F1\": cv_f1},\n                          index=[0])\n\ncv_metrics.T.plot.bar(title=\"Cross-validated classification metrics\",\n                      legend=False);","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:33:08.575653Z","iopub.execute_input":"2022-03-17T05:33:08.575987Z","iopub.status.idle":"2022-03-17T05:33:08.800132Z","shell.execute_reply.started":"2022-03-17T05:33:08.575952Z","shell.execute_reply":"2022-03-17T05:33:08.799252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5.0 Experimentation\n\nFor furthur experimentation and training:\n\n* Collect more data?\n* Try a better model? Like CatBoost or XGBoost?\n* Improve the current models? ","metadata":{}},{"cell_type":"markdown","source":"## 5.1 Feature Importance\n\nFeature importance is another as asking, \"which features contributed most to the outcome of the model and how did they contribute?\"\n\nFinding feature importance is different for each machine learning model.\n\nWe may refer to feature importance for future collecting data.\n","metadata":{}},{"cell_type":"code","source":"#Fit an instance of LR\nclf = LogisticRegression(C=0.20433597178569418,\n                        solver=\"liblinear\")\n\nclf.fit(X_train,y_train);","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:46:40.29013Z","iopub.execute_input":"2022-03-17T05:46:40.29082Z","iopub.status.idle":"2022-03-17T05:46:40.302962Z","shell.execute_reply.started":"2022-03-17T05:46:40.290777Z","shell.execute_reply":"2022-03-17T05:46:40.302042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check coef_\nclf.coef_","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:46:42.347147Z","iopub.execute_input":"2022-03-17T05:46:42.347456Z","iopub.status.idle":"2022-03-17T05:46:42.354028Z","shell.execute_reply.started":"2022-03-17T05:46:42.34742Z","shell.execute_reply":"2022-03-17T05:46:42.35319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Match coef to columns\nfeature_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeature_dict","metadata":{"execution":{"iopub.status.busy":"2022-03-17T05:46:43.523192Z","iopub.execute_input":"2022-03-17T05:46:43.524026Z","iopub.status.idle":"2022-03-17T05:46:43.531307Z","shell.execute_reply.started":"2022-03-17T05:46:43.523974Z","shell.execute_reply":"2022-03-17T05:46:43.530292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualise feature importance\nfeature_df = pd.DataFrame(feature_dict,index=[0])\nfeature_df.T.plot.bar(title=\"Feature Importance\",legend=False)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-03-17T05:46:46.033283Z","iopub.execute_input":"2022-03-17T05:46:46.033593Z","iopub.status.idle":"2022-03-17T05:46:46.341417Z","shell.execute_reply.started":"2022-03-17T05:46:46.033557Z","shell.execute_reply":"2022-03-17T05:46:46.340559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Base on the visualisation, \n* chest pain type(cp)\n* resting electrocardiographic results(restecg) \n* slope of the peak exercise ST segment(slope) \n\nhave strong feature importance.\n\nOn the other hand, sex has the least feature importance.","metadata":{}}]}